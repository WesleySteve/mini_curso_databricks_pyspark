{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando os dados\n",
    "  # assim ele carrega os dados sem o header\n",
    "dfcarros = spark.read.format('csv').load('./data/modelo_carro.csv')\n",
    "display(df_carros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iportando dados com header forma1\n",
    "dfcarros = spark.read.format('csv').option('header', True).load('./data/modelo_carro.csv', sep=',')\n",
    "\n",
    "display(dfcarros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iportando dados com header forma2\n",
    "df_carros = spark.read.format('csv')\n",
    "  .option('header', True)\n",
    "  .option('delimiter', ',')\n",
    "  # .option('encoding', 'utf-8')      \n",
    "  .load('./data/modelo_carro.csv')\n",
    "\n",
    "display(df_carros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usando sql\n",
    "  ### primeiro devemos criar uma tabela temporarioa\n",
    "  ### segundo devemos colocar %sql para fazer a consulta livre na tabela temporaria\n",
    "  ### terceiro mais se quisermos salvar o resultado da consulta sql em um df\n",
    "      - devemos definir um df novo = o resultado da consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando tabela temporaria\n",
    "df_carros.createOrReplaceTempView('carros')\n",
    "df_carros_sql = spark.sql('''\n",
    "                          SELECT id_carro, modelo_carro\n",
    "                          FROM carros\n",
    "                    ''')\n",
    "\n",
    "display(df_carros_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo select com %sql\n",
    "%sql\n",
    "SELECT *\n",
    "FROM carros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo o mesmo processo do sql somente com o spark\n",
    "df_carros_spark = df_carros.select('id_carro', 'modelo_carro')\n",
    "display(df_carros_spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo o mesmo processo anterior colocano apelidos nas colunas (opcao1)\n",
    "df_carros_spark = df_carros.selectExpr('id_carro AS codigo', 'modelo_carro')\n",
    "display(df_carros_spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo o mesmo processo anterior colocano apelidos nas colunas (opcao2)\n",
    "  # from pyspark.sql.functions import col   --> assim importa somente oq vai usar\n",
    "  # from pyspark.sql.functions import *     --> assim importa tudo que o pacote functions tem\n",
    "from pyspark.sql.functions import col\n",
    "df_carros_spark = df_carros.select(col('id_carro').alias('codigo'), 'modelo_carro')\n",
    "display(df_carros_spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
