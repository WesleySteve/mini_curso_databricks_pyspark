{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_1 = spark.createDataFrame(['2021-07-05T10:00:00.000+000', '2020-12-05T10:00:00.000+000'], \"string\").toDF(\"datas_1\")\n",
    "df_data_2 = spark.createDataFrame(['2021-07-05 10:00' '2020-09-02 11:00'], \"string\").toDF(\"datas_2\") \n",
    "df_data_3 = spark.createDataFrame(['05/10/2021', '02/11/2020'], \"string\").toDF(\"datas_3\")\n",
    "df_data_4 = spark.createDataFrame(['05/10/2021 12:50', '02/11/2020 12:00'], \"string\").toDF(\"datas_4\")\n",
    "\n",
    "display(df_data_1)\n",
    "display(df_data_2)\n",
    "display(df_data_3)\n",
    "display(df_data_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_1 = createOrReplaceTempView('datas1')\n",
    "df_data_2 = createOrReplaceTempView('datas2')\n",
    "df_data_3 = createOrReplaceTempView('datas3')\n",
    "df_data_4 = createOrReplaceTempView('datas4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "df_datas_1_tipagem = spark.sql('''\n",
    "                               SELECT TO_TIMESTAMP(datas) AS datas\n",
    "                                FROM datas1\n",
    "                        ''')\n",
    "\n",
    "display(df_datas_1_tipagem)\n",
    "\n",
    "df_datas_2_tipagem = spark.sql('''\n",
    "                               SELECT CAST(datas AS DATE) AS datas\n",
    "                                FROM datas2\n",
    "                        ''')\n",
    "\n",
    "display(df_datas_2_tipagem)\n",
    "\n",
    "\n",
    "df_datas_3_tipagem = spark.sql('''\n",
    "                               SELECT TO_DATE(datas, 'dd/MM/YYYY') AS datas\n",
    "                                FROM datas3\n",
    "                        ''')\n",
    "\n",
    "display(df_datas_3_tipagem)\n",
    "\n",
    "\n",
    "df_datas_4_tipagem = spark.sql('''\n",
    "                               SELECT TO_DATE(datas, 'dd/MM/yyyy HH:mm') AS datas\n",
    "                                FROM datas4\n",
    "                        ''')\n",
    "\n",
    "display(df_datas_4_tipagem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando sql para converter os campos para data\n",
    "%sql\n",
    "SELECT TO_TIMESTAMP(datas) AS datas\n",
    "FROM datas1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando sql para converter os campos para data\n",
    "%sql\n",
    "SELECT CAST(datas AS DATE) AS datas\n",
    "FROM datas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando sql para converter os campos para data\n",
    "%sql\n",
    "SELECT TO_DATE(datas, 'dd/MM/YYYY') AS datas\n",
    "FROM datas3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando sql para converter os campos para data\n",
    "%sql\n",
    "SELECT TO_DATE(datas, 'dd/MM/yyyy HH:mm') AS datas\n",
    "FROM datas4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo o mesmo processo com pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_datas_1_pyspark = df_data_1\n",
    "df_datas_1_pyspark = df_datas_1_pyspark.spark.withColumn(\"datas\", to_timestamp(\"datas\"))\n",
    "\n",
    "display(df_datas_1_pyspark)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "df_datas_2_pyspark = df_data_2\n",
    "df_datas_2_pyspark = df_datas_2_pyspark.spark.withColumn(\"datas\", to_timestamp(\"datas\"))\n",
    "\n",
    "display(df_datas_2_pyspark)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "df_datas_3_pyspark = df_data_3\n",
    "df_datas_3_pyspark = df_datas_3_pyspark.spark.withColumn(\"datas\", to_date(\"datas\", 'dd/MM/yyyy'))\n",
    "\n",
    "display(df_datas_3_pyspark)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "df_datas_4_pyspark = df_data_4\n",
    "df_datas_4_pyspark = df_datas_4_pyspark.spark.withColumn(\"datas\", to_date(\"datas\", 'dd/MM/yyyy HH:mm'))\n",
    "\n",
    "display(df_datas_4_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
